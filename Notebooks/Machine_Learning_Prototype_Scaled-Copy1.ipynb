{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import dayofweek, year, month, hour\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.evaluation import RegressionMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "  .appName('Chicagotaxi') \\\n",
    "  .config('spark.jars.packages', 'com.google.cloud.spark:spark-bigquery-with-dependencies_2.11:0.15.1-beta,com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc2') \\\n",
    "  .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\") \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries from LBGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmlspark.featurize import AssembleFeatures\n",
    "from mmlspark.stages import UDFTransformer, DropColumns\n",
    "from mmlspark.featurize import DataConversion\n",
    "from mmlspark.lightgbm import LightGBMRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a spark dataframe with subset of data from Bigquery.  Removing the filter condition from the below option will give full dataset from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master = spark.read.format('bigquery') \\\n",
    "            .option(\"credentialsFile\", 'key.json') \\\n",
    "            .option('parentproject', 'zeta-treat-276509') \\\n",
    "            .option('project', 'zeta-treat-276509') \\\n",
    "            .option('table', 'bigquery-public-data:chicago_taxi_trips.taxi_trips') \\\n",
    "            .option(\"filter\",\n",
    "                    \"EXTRACT(MONTH from trip_start_timestamp) = 3 and \"\n",
    "                    \"EXTRACT(DAYOFWEEK from trip_start_timestamp) = 3 and \"\n",
    "                    \"EXTRACT(YEAR from trip_start_timestamp) = 2019\") \\\n",
    "            .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Model purpose, lets choose only the below fields\n",
    "1. trip_start_timestamp\n",
    "2. pickup_latitude, pickup_longitude\n",
    "3. dropoff_latitude, dropoff_longitude\n",
    "4. compare\n",
    "5. fare - This field will be our label to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(trip_start_timestamp,TimestampType,true),StructField(pickup_latitude,DoubleType,true),StructField(pickup_longitude,DoubleType,true),StructField(dropoff_latitude,DoubleType,true),StructField(dropoff_longitude,DoubleType,true),StructField(company,StringType,true),StructField(fare,DoubleType,true)))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_master[['trip_start_timestamp','pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude','company','fare']]\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the rows that have blank values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the rows that have fare less than $2.70, which is the minium taxi fare in chicago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df.fare >= 2.70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the given timestamp to CST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "from datetime import datetime\n",
    "get_week_day = udf(lambda z: z.weekday(), IntegerType())\n",
    "get_year = udf(lambda z: z.year, IntegerType())\n",
    "get_month = udf(lambda z: z.month, IntegerType())\n",
    "get_hour = udf(lambda z: z.hour, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = AssembleFeatures(columnsToFeaturize=['pickup_latitude','pickup_longitude','dropoff_latitude', 'dropoff_longitude',\n",
    "                                      'Trip_Day_Of_Week', 'Trip_Year', 'Trip_Month', 'Trip_Hour','company'],\n",
    "                            numberOfFeatures=9)\n",
    "lgbm = LightGBMRegressor(learningRate=0.001,\n",
    "                           numIterations=50,\n",
    "                           featuresCol='features',\n",
    "                           labelCol='fare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGES = [UDFTransformer(inputCol='trip_start_timestamp', outputCol='Trip_Day_Of_Week', udf=get_week_day),\n",
    "          UDFTransformer(inputCol='trip_start_timestamp', outputCol='Trip_Year', udf=get_year),\n",
    "          UDFTransformer(inputCol='trip_start_timestamp', outputCol='Trip_Month', udf=get_month),\n",
    "          UDFTransformer(inputCol='trip_start_timestamp', outputCol='Trip_Hour', udf=get_hour),\n",
    "          DataConversion(cols=['pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude'], convertTo='double'),\n",
    "          DataConversion(cols=['pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude'], convertTo='double'),\n",
    "          DropColumns(cols=['trip_start_timestamp']), assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test) = df.randomSplit([.90, 0.10], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pip = Pipeline(stages=STAGES)\n",
    "model = train_pip.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().overwrite().save('data_prep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "data_prep = PipelineModel.load('data_prep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_train = data_prep.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, pickup_latitude: string, pickup_longitude: string, dropoff_latitude: string, dropoff_longitude: string, company: string, fare: string, Trip_Day_Of_Week: string, Trip_Year: string, Trip_Month: string, Trip_Hour: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_train.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+----------------+-----------------+--------------------+-----+----------------+---------+----------+---------+--------------------+\n",
      "|pickup_latitude|pickup_longitude|dropoff_latitude|dropoff_longitude|             company| fare|Trip_Day_Of_Week|Trip_Year|Trip_Month|Trip_Hour|            features|\n",
      "+---------------+----------------+----------------+-----------------+--------------------+-----+----------------+---------+----------+---------+--------------------+\n",
      "|    41.77887686|   -87.594925439|     41.77887686|    -87.594925439|Taxi Affiliation ...| 4.75|               0|     2019|         3|       19|(17,[1,2,3,4,5,6,...|\n",
      "|   41.874005383|    -87.66351755|    42.009622881|    -87.670166857|Taxi Affiliation ...| 29.5|               0|     2019|         3|       19|(17,[1,2,3,4,5,6,...|\n",
      "|   41.878865584|   -87.625192142|    41.835117986|    -87.618677767|Choice Taxi Assoc...| 13.5|               0|     2019|         3|       19|(17,[1,2,3,4,5,6,...|\n",
      "|   41.878865584|   -87.625192142|    41.944226601|    -87.655998182|Blue Ribbon Taxi ...|15.75|               0|     2019|         3|       19|(17,[1,2,3,4,5,6,...|\n",
      "|   41.880994471|   -87.632746489|    41.892042136|     -87.63186395|Taxi Affiliation ...| 5.75|               0|     2019|         3|       19|(17,[1,2,3,4,5,6,...|\n",
      "+---------------+----------------+----------------+-----------------+--------------------+-----+----------------+---------+----------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(pickup_latitude,DoubleType,true),StructField(pickup_longitude,DoubleType,true),StructField(dropoff_latitude,DoubleType,true),StructField(dropoff_longitude,DoubleType,true),StructField(company,StringType,true),StructField(fare,DoubleType,true),StructField(Trip_Day_Of_Week,IntegerType,true),StructField(Trip_Year,IntegerType,true),StructField(Trip_Month,IntegerType,true),StructField(Trip_Hour,IntegerType,true),StructField(features,VectorUDT,true)))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_train.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGES1 = [assembler, lgbm]\n",
    "train_pip = Pipeline(stages=STAGES1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = train_pip.fit(res_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LightGBMRegressor(labelCol='fare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = lgbm.fit(res_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "class Preprocessdataframe(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Preprocessdataframe, self).__init__()\n",
    "\n",
    "    def _transform(self, df):\n",
    "\n",
    "        df = df.withColumn('trip_start_timestamp_dt',F.to_timestamp(F.unix_timestamp('trip_start_timestamp', 'yyy-MM-dd HH:mm:ss Z').cast('timestamp')))\n",
    "        df = df.withColumn('trip_start_timestamp_cst', F.from_utc_timestamp('trip_start_timestamp_dt', 'CST'))\n",
    "        df = df.withColumn('Trip_Day_Of_Week', dayofweek(df.trip_start_timestamp))\n",
    "        df = df.withColumn('Trip_Year', year(df.trip_start_timestamp))\n",
    "        df = df.withColumn('Trip_Month', month(df.trip_start_timestamp))\n",
    "        df = df.withColumn('Trip_Hour', hour(df.trip_start_timestamp_cst))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data as test and train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(train, test) = df.randomSplit([.90, 0.10], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an assembler and lgbm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess = Preprocessdataframe()\n",
    "assembler = AssembleFeatures(columnsToFeaturize=['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude',\n",
    "                                      'Trip_Day_Of_Week', 'Trip_Year', 'Trip_Month', 'Trip_Hour','company'],\n",
    "                            numberOfFeatures=9)\n",
    "lgbm = LightGBMRegressor(learningRate=0.001,\n",
    "                           numIterations=50,\n",
    "                           featuresCol='features',\n",
    "                           labelCol='fare')\n",
    "STAGES = [preprocess, assembler, lgbm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model on train data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pip = Pipeline(stages=STAGES)\n",
    "model = train_pip.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the fare for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Root Mean Square Error of the precited results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesandpreds = results.rdd.map(lambda p: (float(p.prediction), p.fare))\n",
    "metric = RegressionMetrics(valuesandpreds)\n",
    "print('RMSE for Light GBM is ', metric.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().overwrite().save('linear_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Preprocessdataframe()\n",
    "STAGES1 = [preprocess]\n",
    "train_preprocess_pip = Pipeline(stages=STAGES1)\n",
    "train_preprocess_model = train_preprocess_pip.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocess_model.write().overwrite().save('linear_model_preprocess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "prep_model = PipelineModel.load('linear_model_preprocess')\n",
    "train_preprocessed = prep_model.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGES2 = [assembler, lgbm]\n",
    "train_model_pip = Pipeline(stages=STAGES1)\n",
    "train_model = train_model_pip.fit(train_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocess_model.write().overwrite().save('linear_model_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
