{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import dayofweek, year, month, hour\n",
    "from pyspark.ml import Pipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "  .appName('Chicagotaxi') \\\n",
    "  .config('spark.jars.packages', 'com.google.cloud.spark:spark-bigquery-with-dependencies_2.11:0.15.1-beta') \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a spark dataframe with subset of data from Bigquery.  Removing the filter condition from the below option will give full dataset from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master = spark.read.format('bigquery') \\\n",
    "            .option(\"credentialsFile\", '/Users/karthikeyangurusamy/Documents/GCP/key.json') \\\n",
    "            .option('parentproject', 'zeta-treat-276509') \\\n",
    "            .option('project', 'zeta-treat-276509') \\\n",
    "            .option('table', 'bigquery-public-data:chicago_taxi_trips.taxi_trips') \\\n",
    "            .option(\"filter\",\n",
    "                    \"EXTRACT(MONTH from trip_start_timestamp) = 3 and \"\n",
    "                    \"EXTRACT(DAYOFWEEK from trip_start_timestamp) = 3 and \"\n",
    "                    \"EXTRACT(YEAR from trip_start_timestamp) = 2019\") \\\n",
    "            .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Model purpose, lets choose only the below fields\n",
    "1. trip_start_timestamp\n",
    "2. pickup_latitude, pickup_longitude\n",
    "3. dropoff_latitude, dropoff_longitude\n",
    "4. compare\n",
    "5. fare - This field will be our label to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(trip_start_timestamp,TimestampType,true),StructField(pickup_latitude,DoubleType,true),StructField(pickup_longitude,DoubleType,true),StructField(dropoff_latitude,DoubleType,true),StructField(dropoff_longitude,DoubleType,true),StructField(company,StringType,true),StructField(fare,DoubleType,true)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_master[['trip_start_timestamp','pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude','company','fare']]\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, pickup_latitude: string, pickup_longitude: string, dropoff_latitude: string, dropoff_longitude: string, company: string, fare: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the rows that have blank values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the rows that have fare less than $2.70, which is the minium taxi fare in chicago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df.fare >= 2.70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the given timestamp to CST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('trip_start_timestamp_dt',F.to_timestamp(F.unix_timestamp('trip_start_timestamp', 'yyy-MM-dd HH:mm:ss Z').cast('timestamp')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('trip_start_timestamp_cst', F.from_utc_timestamp('trip_start_timestamp_dt', 'CST'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Day of the week, trip year, trip momnth, trip hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('Trip_Day_Of_Week', dayofweek(df.trip_start_timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('Trip_Year', year(df.trip_start_timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('Trip_Month', month(df.trip_start_timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('Trip_Hour', hour(df.trip_start_timestamp_cst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+----------------+----------------+-----------------+----------------+----+-----------------------+------------------------+----------------+---------+----------+---------+\n",
      "|trip_start_timestamp|pickup_latitude|pickup_longitude|dropoff_latitude|dropoff_longitude|         company|fare|trip_start_timestamp_dt|trip_start_timestamp_cst|Trip_Day_Of_Week|Trip_Year|Trip_Month|Trip_Hour|\n",
      "+--------------------+---------------+----------------+----------------+-----------------+----------------+----+-----------------------+------------------------+----------------+---------+----------+---------+\n",
      "| 2019-03-18 22:15:00|      41.968069|   -87.721559063|    41.983636307|    -87.723583185|       Flash Cab| 7.0|    2019-03-18 22:15:00|     2019-03-18 17:15:00|               2|     2019|         3|       17|\n",
      "| 2019-03-05 05:30:00|    41.96581197|   -87.655878786|     41.96581197|    -87.655878786|       Flash Cab|4.25|    2019-03-05 05:30:00|     2019-03-04 23:30:00|               3|     2019|         3|       23|\n",
      "| 2019-03-26 19:45:00|    41.96581197|   -87.655878786|    41.975170943|    -87.687515515|    City Service|5.25|    2019-03-26 19:45:00|     2019-03-26 14:45:00|               3|     2019|         3|       14|\n",
      "| 2019-03-26 17:45:00|   41.899602111|   -87.633308037|    41.899602111|    -87.633308037|       Flash Cab| 5.5|    2019-03-26 17:45:00|     2019-03-26 12:45:00|               3|     2019|         3|       12|\n",
      "| 2019-03-26 18:30:00|   41.899602111|   -87.633308037|    41.899602111|    -87.633308037|Medallion Leasin|3.25|    2019-03-26 18:30:00|     2019-03-26 13:30:00|               3|     2019|         3|       13|\n",
      "+--------------------+---------------+----------------+----------------+-----------------+----------------+----+-----------------------+------------------------+----------------+---------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a numeric index column corresponding to the 'Company' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "stringIndexer = StringIndexer(inputCol='company', outputCol='Comp_Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+----------------+----------------+-----------------+---------+----+-----------------------+------------------------+----------------+---------+----------+---------+----------+\n",
      "|trip_start_timestamp|pickup_latitude|pickup_longitude|dropoff_latitude|dropoff_longitude|  company|fare|trip_start_timestamp_dt|trip_start_timestamp_cst|Trip_Day_Of_Week|Trip_Year|Trip_Month|Trip_Hour|Comp_Index|\n",
      "+--------------------+---------------+----------------+----------------+-----------------+---------+----+-----------------------+------------------------+----------------+---------+----------+---------+----------+\n",
      "| 2019-03-05 02:00:00|      41.968069|   -87.721559063|       41.968069|    -87.721559063|Flash Cab|3.75|    2019-03-05 02:00:00|     2019-03-04 20:00:00|               3|     2019|         3|       20|       1.0|\n",
      "| 2019-03-04 19:00:00|      41.968069|   -87.721559063|       41.968069|    -87.721559063|Flash Cab|7.75|    2019-03-04 19:00:00|     2019-03-04 13:00:00|               2|     2019|         3|       13|       1.0|\n",
      "| 2019-03-04 21:30:00|      41.968069|   -87.721559063|       41.968069|    -87.721559063|Flash Cab| 4.5|    2019-03-04 21:30:00|     2019-03-04 15:30:00|               2|     2019|         3|       15|       1.0|\n",
      "| 2019-03-05 09:30:00|      41.968069|   -87.721559063|       41.968069|    -87.721559063| Sun Taxi|3.25|    2019-03-05 09:30:00|     2019-03-05 03:30:00|               3|     2019|         3|        3|       3.0|\n",
      "| 2019-03-05 18:30:00|      41.968069|   -87.721559063|       41.968069|    -87.721559063|Flash Cab|4.25|    2019-03-05 18:30:00|     2019-03-05 12:30:00|               3|     2019|         3|       12|       1.0|\n",
      "+--------------------+---------------+----------------+----------------+-----------------+---------+----+-----------------------+------------------------+----------------+---------+----------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updated_data = stringIndexer.fit(df).transform(df)\n",
    "updated_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert all the numerical data columns to a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_array =  np.array(updated_data.select('pickup_latitude','pickup_longitude','dropoff_latitude',\n",
    "                                        'dropoff_longitude','Trip_Day_Of_Week','Trip_Year',\n",
    "                                        'Trip_Month','Trip_Hour','fare').collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Minimum Maximum Scalar to scale the numerical features and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.81100295e-01, 5.07141618e-01, 9.09029009e-01, 5.01797002e-01,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.39130435e-01,\n",
       "       4.72320885e-04])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_array = scaler.fit_transform(data_array)\n",
    "data_array[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets collect the categorical index column to a separate numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_data = np.array(updated_data.select('Comp_Index').collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(191210, 9)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(191210, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply one hot encoding to categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "# one hot encode\n",
    "encoded = to_categorical(cat_data)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate the scaled numerical data column and one hot encoded column to get to the final numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = np.concatenate((data_array[:,:-1],encoded,data_array[:,-1].reshape(191210,1)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(191210, 58)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(final_data)\n",
    "test, training = final_data[:10000,:], final_data[10000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(181210, 58)\n",
      "(10000, 58)\n"
     ]
    }
   ],
   "source": [
    "print(training.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(181210, 57) (181210,) (10000, 57) (10000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = training[:,:-1]\n",
    "y_train = training[:,-1]\n",
    "X_test = test[:,:-1]\n",
    "y_test = test[:,-1]\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and compile a Keras model to apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(57,)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='MeanSquaredError',\n",
    "              metrics=[tf.keras.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training data set through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5663/5663 [==============================] - 7s 1ms/step - loss: 5.0084e-05 - root_mean_squared_error: 0.0071 - val_loss: 5.5926e-06 - val_root_mean_squared_error: 0.0024\n",
      "Epoch 2/5\n",
      "5663/5663 [==============================] - 6s 1ms/step - loss: 3.9885e-05 - root_mean_squared_error: 0.0063 - val_loss: 5.4038e-06 - val_root_mean_squared_error: 0.0023\n",
      "Epoch 3/5\n",
      "5663/5663 [==============================] - 6s 1ms/step - loss: 3.9484e-05 - root_mean_squared_error: 0.0063 - val_loss: 5.7732e-06 - val_root_mean_squared_error: 0.0024\n",
      "Epoch 4/5\n",
      "5663/5663 [==============================] - 6s 1ms/step - loss: 3.9369e-05 - root_mean_squared_error: 0.0063 - val_loss: 6.0027e-06 - val_root_mean_squared_error: 0.0025\n",
      "Epoch 5/5\n",
      "5663/5663 [==============================] - 6s 1ms/step - loss: 3.9387e-05 - root_mean_squared_error: 0.0063 - val_loss: 5.3774e-06 - val_root_mean_squared_error: 0.0023\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, y_train,\n",
    "          batch_size=32, epochs=5,\n",
    "          validation_data=(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
